{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYvq_FZr77ZQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import LinearLR\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import gc\n",
        "import os\n",
        "import imageio\n",
        "import yaml\n",
        "from torch.optim import Adam\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "import requests\n",
        "import nbconvert\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File upload (Make sure 'Image-01.png' is uploaded to /content/)\n",
        "image_file_name = \"Image-01.png\"\n",
        "image_path = os.path.join(\"/content/\", image_file_name) # Basic upload path\n",
        "\n",
        "# Open image\n",
        "try:\n",
        "    original_image = Image.open(image_path)\n",
        "    print(f\"'{image_file_name}' Successfully opened.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Cannot find image. Please upload '{image_file_name}' to /content/\")\n",
        "except Image.UnidentifiedImageError:\n",
        "    print(f\"{image_path} is not a valid image file.\")\n",
        "except Exception as e:\n",
        "    print(f\"Undefined error: {e}\")"
      ],
      "metadata": {
        "id": "3Ynv2RmZ79KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [CODE BLOCK 1: Function Definition]\n",
        "\n",
        "def generate_2D_gaussian_splatting(\n",
        "    kernel_size, scale, rotation, coords, colours, image_size=(256, 256, 3),\n",
        "    device=\"cpu\", filter_type=None, debug_print=False):\n",
        "    \"\"\"\n",
        "    Generates a 2D image by splatting multiple Gaussians.\n",
        "\n",
        "    Args:\n",
        "        kernel_size (int): Size of the square kernel for each Gaussian.\n",
        "        scale (torch.Tensor): Tensor of shape (N, 2) representing the x and y scales of N Gaussians.\n",
        "        rotation (torch.Tensor): Tensor of shape (N) representing the rotation angle (radians) of N Gaussians.\n",
        "        coords (torch.Tensor): Tensor of shape (N, 2) representing the normalized ([-1, 1]) center coordinates (x, y) of N Gaussians.\n",
        "        colours (torch.Tensor): Tensor of shape (N, 3) representing the RGB (with alpha pre-multiplied) colors of N Gaussians.\n",
        "        image_size (tuple, optional): Target image dimensions (height, width, channels). Defaults to (256, 256, 3).\n",
        "        device (str, optional): Device to run computations on ('cpu' or 'cuda'). Defaults to \"cpu\".\n",
        "        filter_type (str, optional): Type of anti-aliasing filter.\n",
        "                                     Can be 'low_pass', 'ewa', or None. Defaults to None.\n",
        "        debug_print (bool, optional): If True, prints covariance matrix changes for debugging.\n",
        "                                      Defaults to False.\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = colours.shape[0]\n",
        "\n",
        "    # Ensure scale and rotation have the correct shape\n",
        "    scale = scale.view(batch_size, 2)\n",
        "    rotation = rotation.view(batch_size)\n",
        "\n",
        "    # Compute the components of the covariance matrix\n",
        "    cos_rot = torch.cos(rotation)\n",
        "    sin_rot = torch.sin(rotation)\n",
        "\n",
        "    R = torch.stack([\n",
        "        torch.stack([cos_rot, -sin_rot], dim=-1),\n",
        "        torch.stack([sin_rot, cos_rot], dim=-1)\n",
        "    ], dim=-2)\n",
        "\n",
        "    S = torch.diag_embed(scale)\n",
        "\n",
        "    # Calculate the original covariance matrix: Î£_splat = R @ S @ S @ R^T\n",
        "    covariance = R @ S @ S @ R.transpose(-1, -2)\n",
        "\n",
        "    # For debugging: store the original covariance before filtering\n",
        "    if debug_print:\n",
        "        covariance_before = covariance.clone()\n",
        "\n",
        "    # --- Apply anti-aliasing filters ---\n",
        "    # This section modifies the 'covariance' matrix based on the chosen filter_type\n",
        "    if filter_type == 'low_pass':\n",
        "    # Implement Low-Pass Filter:\n",
        "    # Goal: If a Gaussian is too small (causing aliasing), make it slightly larger (blurrier).\n",
        "    # 1. Determine a 'threshold' for the Gaussian's size (e.g., related to pixel dimensions).\n",
        "    # 2. Calculate the eigenvalues of the 'covariance' matrix to determine its spread.\n",
        "    # 3. Identify Gaussians whose spread (eigenvalues) falls below the threshold.\n",
        "    # 4. For these identified Gaussians, add a small isotropic (uniform) blur amount to their 'covariance' matrix (e.g., add 'blur_amount_square * torch.eye(2)' to it).\n",
        "\n",
        "        pass # Placeholder, remove this line when implementing\n",
        "\n",
        "    elif filter_type == 'ewa':\n",
        "   # Implement EWA Filter:\n",
        "   # Goal: Convolve the Gaussian with a pixel filter Gaussian.\n",
        "   # This conceptually means the final rendered Gaussian's covariance is the sum\n",
        "   # of the original Gaussian's covariance and the pixel filter's covariance.\n",
        "   # 1. Define a 'pixel_radius' (e.g., 0.5 for a standard pixel).\n",
        "   # 2. Create the covariance matrix for the pixel filter (Sigma_pixel) based on 'pixel_radius'.\n",
        "   #    It should be an isotropic (circular) blur.\n",
        "   # 3. Add Sigma_pixel to the original 'covariance' matrix. This applies to all Gaussians.\n",
        "\n",
        "        pass # Placeholder, remove this line when implementing\n",
        "\n",
        "    # For debugging: print comparison of covariance matrices and eigenvalues\n",
        "    if debug_print:\n",
        "        scales_det = scale[:, 0] * scale[:, 1]\n",
        "        target_idx = torch.argmin(scales_det)\n",
        "\n",
        "        print(f\"--- Debugging Anti-Aliasing Filter: '{filter_type if filter_type else 'None'}' ---\\n\")\n",
        "        print(f\"Target Gaussian Index (smallest by scale): {target_idx.item()}\")\n",
        "        print(\"Covariance Before Filtering (Smallest Gaussian):\\n\", covariance_before[target_idx].detach().cpu().numpy())\n",
        "        print(\"Covariance After Filtering (Smallest Gaussian):\\n\", covariance[target_idx].detach().cpu().numpy())\n",
        "\n",
        "        eig_before = torch.linalg.eigvalsh(covariance_before[target_idx])\n",
        "        eig_after = torch.linalg.eigvalsh(covariance[target_idx])\n",
        "        print(\"Eigenvalues Before Filtering:\\n\", eig_before.detach().cpu().numpy())\n",
        "        print(\"Eigenvalues After Filtering:\\n\", eig_after.detach().cpu().numpy())\n",
        "        print(\"-\" * 60 + \"\\n\")\n",
        "\n",
        "    # Compute inverse covariance (will use the potentially filtered covariance)\n",
        "    inv_covariance = torch.inverse(covariance)\n",
        "\n",
        "    # Create the kernel grid\n",
        "    x = torch.linspace(-5, 5, kernel_size, device=device)\n",
        "    y = torch.linspace(-5, 5, kernel_size, device=device)\n",
        "    xx, yy = torch.meshgrid(x, y, indexing='ij')\n",
        "    xy = torch.stack([xx, yy], dim=-1).unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
        "\n",
        "    # Calculate the exponent part of the Gaussian PDF\n",
        "    z = torch.einsum('bxyi,bij,bxyj->bxy', xy, -0.5 * inv_covariance, xy)\n",
        "\n",
        "    # Calculate the full Gaussian PDF (integral is 1). This is the correct base intensity.\n",
        "    kernel = torch.exp(z) / (2 * torch.tensor(np.pi, device=device) * torch.sqrt(torch.det(covariance))).view(batch_size, 1, 1)\n",
        "\n",
        "    # Use the 'kernel' (PDF values) directly.\n",
        "    # The 'colours' parameter already includes alpha from the caller (colours_with_alpha).\n",
        "    kernel_rgb_base = kernel.unsqueeze(1).expand(-1, 3, -1, -1)\n",
        "\n",
        "    # Add padding to match target image size\n",
        "    pad_h = image_size[0] - kernel_size\n",
        "    pad_w = image_size[1] - kernel_size\n",
        "\n",
        "    if pad_h < 0 or pad_w < 0:\n",
        "        raise ValueError(\"Kernel size should be smaller or equal to the image size.\")\n",
        "\n",
        "    # Padding dimensions: (left, right, top, bottom)\n",
        "    padding = (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2)\n",
        "    kernel_rgb_padded = F.pad(kernel_rgb_base, padding, \"constant\", 0)\n",
        "\n",
        "    # Create affine grid for translating the kernel to Gaussian's coordinates\n",
        "    b, c, h, w = kernel_rgb_padded.shape\n",
        "    theta = torch.zeros(b, 2, 3, dtype=torch.float32, device=device)\n",
        "    theta[:, 0, 0] = 1.0\n",
        "    theta[:, 1, 1] = 1.0\n",
        "    theta[:, :, 2] = coords\n",
        "\n",
        "    grid = F.affine_grid(theta, size=(b, c, h, w), align_corners=True)\n",
        "    kernel_rgb_padded_translated = F.grid_sample(kernel_rgb_padded, grid, align_corners=True)\n",
        "\n",
        "    # Apply colors (which already include alpha from the input 'colours') and sum the layers\n",
        "    # rgb_values_reshaped holds (RGB * Alpha) from the input 'colours' parameter\n",
        "    rgb_values_reshaped = colours.unsqueeze(-1).unsqueeze(-1)\n",
        "    final_image_layers = rgb_values_reshaped * kernel_rgb_padded_translated\n",
        "    final_image = final_image_layers.sum(dim=0)\n",
        "\n",
        "    # Clamp values to [0, 1] and permute dimensions for imshow (H, W, C)\n",
        "    final_image = torch.clamp(final_image, 0, 1)\n",
        "    final_image = final_image.permute(1, 2, 0)\n",
        "\n",
        "    return final_image"
      ],
      "metadata": {
        "id": "xZE5rDcQ7_Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_window(window_size, channel):\n",
        "    def gaussian(window_size, sigma):\n",
        "        gauss = torch.exp(torch.tensor([-(x - window_size//2)**2/float(2*sigma**2) for x in range(window_size)]))\n",
        "        return gauss/gauss.sum()\n",
        "\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = torch.autograd.Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
        "\n",
        "    return window\n",
        "\n",
        "\n",
        "\n",
        "def ssim(img1, img2, window_size=11, size_average=True):\n",
        "\n",
        "    (_, _, channel) = img1.size()\n",
        "\n",
        "    img1 = img1.unsqueeze(0).permute(0, 3, 1, 2)\n",
        "    img2 = img2.unsqueeze(0).permute(0, 3, 1, 2)\n",
        "\n",
        "\n",
        "    # Parameters for SSIM\n",
        "    C1 = 0.01**2\n",
        "    C2 = 0.03**2\n",
        "\n",
        "    window = create_window(window_size, channel)\n",
        "\n",
        "    if img1.is_cuda:\n",
        "        window = window.cuda(img1.get_device())\n",
        "    window = window.type_as(img1)\n",
        "\n",
        "    mu1 = F.conv2d(img1, window, padding=window_size//2, groups=channel)\n",
        "    mu2 = F.conv2d(img2, window, padding=window_size//2, groups=channel)\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1*img1, window, padding=window_size//2, groups=channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2*img2, window, padding=window_size//2, groups=channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1*img2, window, padding=window_size//2, groups=channel) - mu1_mu2\n",
        "\n",
        "    SSIM_numerator = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))\n",
        "    SSIM_denominator = ((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
        "    SSIM = SSIM_numerator / SSIM_denominator\n",
        "\n",
        "    return torch.clamp((1 - SSIM) / 2, 0, 1)\n",
        "\n",
        "def d_ssim_loss(img1, img2, window_size=11, size_average=True):\n",
        "    return ssim(img1, img2, window_size, size_average).mean()\n",
        "\n",
        "# Combined Loss\n",
        "def combined_loss(pred, target, lambda_param=0.5):\n",
        "    l1loss = nn.L1Loss()\n",
        "    return (1 - lambda_param) * l1loss(pred, target) + lambda_param * d_ssim_loss(pred, target)"
      ],
      "metadata": {
        "id": "tEu1pMln7_Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the config.yml file\n",
        "with open('config.yml', 'r') as config_file:\n",
        "    config = yaml.safe_load(config_file)\n",
        "\n",
        "# Extract values from the loaded config\n",
        "KERNEL_SIZE = config[\"KERNEL_SIZE\"]\n",
        "image_size = tuple(config[\"image_size\"])\n",
        "primary_samples = config[\"primary_samples\"]\n",
        "backup_samples = config[\"backup_samples\"]\n",
        "num_epochs = config[\"num_epochs\"]\n",
        "densification_interval = config[\"densification_interval\"]\n",
        "learning_rate = config[\"learning_rate\"]\n",
        "image_file_name = config[\"image_file_name\"]\n",
        "display_interval = config[\"display_interval\"]\n",
        "grad_threshold = config[\"gradient_threshold\"]\n",
        "gauss_threshold = config[\"gaussian_threshold\"]\n",
        "display_loss = config[\"display_loss\"]"
      ],
      "metadata": {
        "id": "a0c6fB567_ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def give_required_data(input_coords, image_size):\n",
        "\n",
        "  # normalising pixel coordinates [-1,1]\n",
        "  coords = torch.tensor(input_coords / [image_size[0],image_size[1]], device=device).float()\n",
        "  center_coords_normalized = torch.tensor([0.5, 0.5], device=device).float()\n",
        "  coords = (center_coords_normalized - coords) * 2.0\n",
        "\n",
        "  # Fetching the colour of the pixels in each coordinates\n",
        "  colour_values = [image_array[coord[1], coord[0]] for coord in input_coords]\n",
        "  colour_values_np = np.array(colour_values)\n",
        "  colour_values_tensor =  torch.tensor(colour_values_np, device=device).float()\n",
        "\n",
        "  return colour_values_tensor, coords"
      ],
      "metadata": {
        "id": "86jRm43u7_cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_samples = primary_samples + backup_samples\n",
        "\n",
        "PADDING = KERNEL_SIZE // 2\n",
        "image_path = image_file_name\n",
        "original_image = Image.open(image_path)\n",
        "original_image = original_image.resize((image_size[0],image_size[0]))\n",
        "original_image = original_image.convert('RGB')\n",
        "original_array = np.array(original_image)\n",
        "original_array = original_array / 255.0\n",
        "width, height, _ = original_array.shape\n",
        "\n",
        "image_array = original_array\n",
        "target_tensor = torch.tensor(image_array, dtype=torch.float32, device=device)\n",
        "coords = np.random.randint(0, [width, height], size=(num_samples, 2))\n",
        "random_pixel_means = torch.tensor(coords, device=device)\n",
        "pixels = [image_array[coord[1], coord[0]] for coord in coords]\n",
        "pixels_np = np.array(pixels)\n",
        "random_pixels =  torch.tensor(pixels_np, device=device)\n",
        "\n",
        "colour_values, pixel_coords = give_required_data(coords, image_size)\n",
        "\n",
        "colour_values = torch.logit(colour_values)\n",
        "pixel_coords = torch.atanh(pixel_coords)\n",
        "\n",
        "scale_values = torch.logit(torch.rand(num_samples, 2, device=device))\n",
        "rotation_values = torch.atanh(2 * torch.rand(num_samples, 1, device=device) - 1)\n",
        "alpha_values = torch.logit(torch.rand(num_samples, 1, device=device))\n",
        "W_values = torch.cat([scale_values, rotation_values, alpha_values, colour_values, pixel_coords], dim=1)\n"
      ],
      "metadata": {
        "id": "-sNa7UZY7_fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "starting_size = primary_samples\n",
        "left_over_size = backup_samples\n",
        "persistent_mask = torch.cat([torch.ones(starting_size, dtype=bool),torch.zeros(left_over_size, dtype=bool)], dim=0)\n",
        "current_marker = starting_size"
      ],
      "metadata": {
        "id": "epzs-soK8H56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get current date and time as string\n",
        "now = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
        "\n",
        "# Create a directory with the current date and time as its name\n",
        "directory = f\"{now}\"\n",
        "os.makedirs(directory, exist_ok=True)"
      ],
      "metadata": {
        "id": "ybYif48S8K66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = nn.Parameter(W_values)\n",
        "optimizer = Adam([W], lr=learning_rate)\n",
        "scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=1e-8, total_iters=num_epochs)\n",
        "loss_history = []"
      ],
      "metadata": {
        "id": "NXW7NWdj8K9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [CODE BLOCK for Training Loop]\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Find indices to remove and update the persistent mask (Pruning logic)\n",
        "    if epoch % (densification_interval + 1) == 0 and epoch > 0:\n",
        "        indices_to_remove = (torch.sigmoid(W[:, 3]) < 0.01).nonzero(as_tuple=True)[0]\n",
        "\n",
        "        if len(indices_to_remove) > 0:\n",
        "          print(f\"Number of pruned points: {len(indices_to_remove)}\")\n",
        "\n",
        "        persistent_mask[indices_to_remove] = False\n",
        "\n",
        "        # Zero-out parameters and their gradients for pruned Gaussians\n",
        "        W.data[~persistent_mask] = 0.0\n",
        "\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Select active Gaussians based on persistent mask\n",
        "    output = W[persistent_mask]\n",
        "\n",
        "    batch_size = output.shape[0]\n",
        "\n",
        "    # Decode Gaussian parameters from the output tensor\n",
        "    scale = torch.sigmoid(output[:, 0:2])\n",
        "    rotation = np.pi / 2 * torch.tanh(output[:, 2]) # Scale rotation to [-pi/2, pi/2]\n",
        "    alpha = torch.sigmoid(output[:, 3])\n",
        "    colours = torch.sigmoid(output[:, 4:7])\n",
        "    pixel_coords = torch.tanh(output[:, 7:9]) # Coords are in [-1, 1] range\n",
        "\n",
        "    colours_with_alpha  = colours * alpha.view(batch_size, 1)\n",
        "\n",
        "    # --- Main Training Loop Rendering Call ---\n",
        "    # The model is trained based on a BASELINE (no filter) output.\n",
        "    # debug_print is OFF by default here for performance.\n",
        "    g_tensor_batch = generate_2D_gaussian_splatting(\n",
        "        KERNEL_SIZE, scale, rotation, pixel_coords, colours_with_alpha,\n",
        "        image_size, device=device,\n",
        "        filter_type=None, # <--- Model is trained with NO FILTER (Baseline)\n",
        "        debug_print=False  # Keep debug_print off during main training\n",
        "    )\n",
        "    # Calculate loss against the target image\n",
        "    loss = combined_loss(g_tensor_batch, target_tensor, lambda_param=0.2)\n",
        "\n",
        "    optimizer.zero_grad() # Clear gradients before backpropagation\n",
        "    loss.backward()       # Perform backpropagation to compute gradients\n",
        "\n",
        "    # Apply zeroing out of gradients for pruned Gaussians\n",
        "    if persistent_mask is not None:\n",
        "        W.grad.data[~persistent_mask] = 0.0\n",
        "\n",
        "    # Densification logic (Splitting and Cloning)\n",
        "    if epoch % densification_interval == 0 and epoch > 0:\n",
        "\n",
        "      # Calculate the norm of gradients\n",
        "      gradient_norms = torch.norm(W.grad[persistent_mask][:, 7:9], dim=1, p=2) # Positional gradients\n",
        "      gaussian_norms = torch.norm(torch.sigmoid(W.data[persistent_mask][:, 0:2]), dim=1, p=2) # Scale norms\n",
        "\n",
        "      sorted_grads, sorted_grads_indices = torch.sort(gradient_norms, descending=True)\n",
        "      sorted_gauss, sorted_gauss_indices = torch.sort(gaussian_norms, descending=True)\n",
        "\n",
        "      large_gradient_mask = (sorted_grads > grad_threshold)\n",
        "      large_gradient_indices = sorted_grads_indices[large_gradient_mask]\n",
        "\n",
        "      large_gauss_mask = (sorted_gauss > gauss_threshold)\n",
        "      large_gauss_indices = sorted_gauss_indices[large_gauss_mask]\n",
        "\n",
        "      common_indices_mask = torch.isin(large_gradient_indices, large_gauss_indices)\n",
        "      common_indices = large_gradient_indices[common_indices_mask]\n",
        "      distinct_indices = large_gradient_indices[~common_indices_mask] # Those with large gradient but small scale\n",
        "\n",
        "      # Split points with large coordinate gradient and large gaussian values and descale their gaussian\n",
        "      if len(common_indices) > 0:\n",
        "        print(f\"Number of splitted points: {len(common_indices)}\")\n",
        "        start_index = current_marker + 1\n",
        "        end_index = current_marker + 1 + len(common_indices)\n",
        "        persistent_mask[start_index: end_index] = True\n",
        "        W.data[start_index:end_index, :] = W.data[common_indices, :]\n",
        "        scale_reduction_factor = 1.6\n",
        "        W.data[start_index:end_index, 0:2] /= scale_reduction_factor\n",
        "        W.data[common_indices, 0:2] /= scale_reduction_factor\n",
        "        current_marker = current_marker + len(common_indices)\n",
        "\n",
        "      # Clone points with large coordinate gradient but small gaussian values\n",
        "      if len(distinct_indices) > 0:\n",
        "\n",
        "        print(f\"Number of cloned points: {len(distinct_indices)}\")\n",
        "        start_index = current_marker + 1\n",
        "        end_index = current_marker + 1 + len(distinct_indices)\n",
        "        persistent_mask[start_index: end_index] = True\n",
        "        W.data[start_index:end_index, :] = W.data[distinct_indices, :]\n",
        "\n",
        "        # Calculate the movement direction based on the positional gradient\n",
        "        positional_gradients = W.grad[distinct_indices, 7:9]\n",
        "        gradient_magnitudes = torch.norm(positional_gradients, dim=1, keepdim=True)\n",
        "        normalized_gradients = positional_gradients / (gradient_magnitudes + 1e-8)\n",
        "\n",
        "        step_size = 0.01\n",
        "\n",
        "        # Move the cloned Gaussians slightly in gradient direction\n",
        "        W.data[start_index:end_index, 7:9] += step_size * normalized_gradients\n",
        "\n",
        "        current_marker = current_marker + len(distinct_indices)\n",
        "\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "    # --- Original Visualization Step ---\n",
        "    # This visualization shows the output of the 'g_tensor_batch' (trained model output)\n",
        "    # and the Ground Truth. It does not perform additional filter comparisons here.\n",
        "    if epoch % display_interval == 0:\n",
        "        num_subplots = 3 if display_loss else 2 # Default to 2 if loss is not displayed\n",
        "        fig_size_width = 18 if display_loss else 12 # Default to 12 if loss is not displayed\n",
        "\n",
        "        # Create subplots for generated image, ground truth, and optionally loss\n",
        "        fig, ax = plt.subplots(1, num_subplots, figsize=(fig_size_width, 6))\n",
        "\n",
        "        # Convert generated tensor to numpy for plotting\n",
        "        generated_array = g_tensor_batch.cpu().detach().numpy()\n",
        "\n",
        "        # Plot generated image (main output of the training loop)\n",
        "        ax[0].imshow(generated_array)\n",
        "        ax[0].set_title('2D Gaussian Splatting (Trained Baseline)') # Updated title\n",
        "        ax[0].axis('off')\n",
        "\n",
        "        # Plot Ground Truth image\n",
        "        ax[1].imshow(target_tensor.cpu().numpy())\n",
        "        ax[1].set_title('Ground Truth')\n",
        "        ax[1].axis('off')\n",
        "\n",
        "        # Optionally plot Loss vs Epochs\n",
        "        if display_loss:\n",
        "          ax[2].plot(range(epoch + 1), loss_history[:epoch + 1])\n",
        "          ax[2].set_title('Loss vs. Epochs (Trained with Baseline)') # Updated title\n",
        "          ax[2].set_xlabel('Epoch')\n",
        "          ax[2].set_ylabel('Loss')\n",
        "          ax[2].set_xlim(0, num_epochs)\n",
        "\n",
        "        # Adjust layout and show plot\n",
        "        plt.subplots_adjust(wspace=0.1)\n",
        "        plt.pause(0.1) # Short pause to update plot in Colab\n",
        "\n",
        "        # Save the generated image\n",
        "        img = Image.fromarray((generated_array * 255).astype(np.uint8))\n",
        "        filename = f\"{epoch}.jpg\"\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        img.save(file_path)\n",
        "        # fig.savefig(file_path, bbox_inches='tight') # Uncomment to save the matplotlib figure directly\n",
        "\n",
        "        # Clear and close plot to free up memory\n",
        "        plt.clf()\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, on {len(output)} points\")"
      ],
      "metadata": {
        "id": "iMvl2zw_8K_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Filter Comparison After Training]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np # Ensure numpy is imported if not already global\n",
        "\n",
        "print(\"--- Generating Comparison Renders from the Trained Model ---\")\n",
        "\n",
        "# Ensure global variables from previous cells are accessible:\n",
        "# KERNEL_SIZE, image_size, device\n",
        "# W (the trained Gaussian parameters after the main training loop)\n",
        "# persistent_mask (to select active Gaussians from W)\n",
        "# target_tensor (the original ground truth image)\n",
        "# generate_2D_gaussian_splatting function (defined in a previous cell)\n",
        "\n",
        "# 1. Extract final active Gaussian parameters from the trained model (W)\n",
        "final_output = W[persistent_mask]\n",
        "\n",
        "final_batch_size = final_output.shape[0]\n",
        "\n",
        "# Decode parameters using the same logic as in the training loop\n",
        "final_scale = torch.sigmoid(final_output[:, 0:2])\n",
        "final_rotation = np.pi / 2 * torch.tanh(final_output[:, 2])\n",
        "final_alpha = torch.sigmoid(final_output[:, 3])\n",
        "final_colours = torch.sigmoid(final_output[:, 4:7])\n",
        "final_pixel_coords = torch.tanh(final_output[:, 7:9])\n",
        "\n",
        "final_colours_with_alpha = final_colours * final_alpha.view(final_batch_size, 1)\n",
        "\n",
        "# 2. Render images with different filter types using the FINAL trained parameters\n",
        "#    debug_print=True is enabled to show numerical changes in covariance for the smallest Gaussian.\n",
        "\n",
        "# A. Baseline Render (No Filter)\n",
        "print(\"Rendering Baseline (No Filter) from trained model...\")\n",
        "compare_baseline_render = generate_2D_gaussian_splatting(\n",
        "    KERNEL_SIZE, final_scale, final_rotation, final_pixel_coords, final_colours_with_alpha,\n",
        "    image_size, device=device,\n",
        "    filter_type=None,       # Apply no filter\n",
        "    debug_print=True        # Enable debug prints for this comparison\n",
        ")\n",
        "\n",
        "# B. Low-Pass Filter Render\n",
        "print(\"Rendering with Low-Pass Filter from trained model...\")\n",
        "compare_low_pass_render = generate_2D_gaussian_splatting(\n",
        "    KERNEL_SIZE, final_scale, final_rotation, final_pixel_coords, final_colours_with_alpha,\n",
        "    image_size, device=device,\n",
        "    filter_type='low_pass', # Apply Low-Pass filter\n",
        "    debug_print=True\n",
        ")\n",
        "\n",
        "# C. EWA Filter Render\n",
        "print(\"Rendering with EWA Filter from trained model...\")\n",
        "compare_ewa_render = generate_2D_gaussian_splatting(\n",
        "    KERNEL_SIZE, final_scale, final_rotation, final_pixel_coords, final_colours_with_alpha,\n",
        "    image_size, device=device,\n",
        "    filter_type='ewa',      # Apply EWA filter\n",
        "    debug_print=True\n",
        ")\n",
        "\n",
        "# 3. Visualize the comparison\n",
        "fig_compare, axes_compare = plt.subplots(1, 4, figsize=(20, 6))\n",
        "\n",
        "# Plot Baseline Render\n",
        "axes_compare[0].imshow(compare_baseline_render.detach().cpu().numpy())\n",
        "axes_compare[0].set_title('Final Render:\\nBaseline (Trained Model)')\n",
        "axes_compare[0].axis('off')\n",
        "\n",
        "# Plot Low-Pass Filtered Render\n",
        "axes_compare[1].imshow(compare_low_pass_render.detach().cpu().numpy())\n",
        "axes_compare[1].set_title('Final Render:\\nLow-Pass Filter')\n",
        "axes_compare[1].axis('off')\n",
        "\n",
        "# Plot EWA Filtered Render\n",
        "axes_compare[2].imshow(compare_ewa_render.detach().cpu().numpy())\n",
        "axes_compare[2].set_title('Final Render:\\nEWA Filter')\n",
        "axes_compare[2].axis('off')\n",
        "\n",
        "# Plot Ground Truth\n",
        "axes_compare[3].imshow(target_tensor.cpu().numpy())\n",
        "axes_compare[3].set_title('Ground Truth')\n",
        "axes_compare[3].axis('off')\n",
        "\n",
        "plt.suptitle(\"Final Model Output: Anti-Aliasing Filter Comparison\", fontsize=16, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Comparison Rendering Complete ---\")"
      ],
      "metadata": {
        "id": "dSj2fkZB43-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "In this assignment, you've observed the rendering process and result with baseline, baseline + Low-pass filter, baseline + EWA filter and ground truth."
      ],
      "metadata": {
        "id": "xIBXhqJR164V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Q1. Describe the specific aliasing artifacts you observed in the Baseline renderings (e.g., jagged edges, flickering, pixelation), particularly around smaller or rapidly changing Gaussians."
      ],
      "metadata": {
        "id": "0Lsphjm63WmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A1. [Answer here]"
      ],
      "metadata": {
        "id": "UD1CauVW4ICA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Visually compare how the Low-Pass and EWA filters modified these artifacts. What were the key visual differences between the Low-Pass and EWA filtered outputs, especially in terms of smoothness, detail preservation, and the overall appearance of both large and small Gaussians?"
      ],
      "metadata": {
        "id": "t_B6bGhQ3bgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A2. [Answer here]"
      ],
      "metadata": {
        "id": "fDz3z-vK4M0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "notebook_name = 'BaseCode_2DGS.ipynb'\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/{notebook_name}\" \"/content/temp_notebook.ipynb\"\n",
        "\n",
        "output_dir = '/content/gdrive/MyDrive/Colab Notebooks/'\n",
        "output_html_name = 'MyNotebook_output.html'\n",
        "\n",
        "!jupyter nbconvert --to html \"/content/temp_notebook.ipynb\" --output-dir=\"{output_dir}\" --output=\"{output_html_name}\"\n",
        "\n",
        "print(f\"HTML file saved to: {output_dir}{output_html_name}\")"
      ],
      "metadata": {
        "id": "49yzCbCre7pB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}